{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.disable(logging.CRITICAL)\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from pyannote.metrics.diarization import DiarizationErrorRate\n",
    "\n",
    "from nemo.utils import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.nemo_util import *\n",
    "# from model.NeMo_diarizer import NeMoDiarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.nemo_util import *\n",
    "\n",
    "class NeMoDiarizer(ClusteringDiarizer):\n",
    "    def __init__(self, cfg: DictConfig, speaker_model=None):\n",
    "        super().__init__(cfg, speaker_model)\n",
    "        self.output_dir = cfg.diarizer.out_dir\n",
    "        self.rttm_dir = os.path.join(self.output_dir, 'pred_rttms')\n",
    "        self.json_dir = os.path.join(self.output_dir, 'pred_json')\n",
    "\n",
    "        if not os.path.exists(self.output_dir):\n",
    "            os.mkdir(self.output_dir)\n",
    "        if not os.path.exists(self.json_dir):\n",
    "            os.mkdir(self.json_dir)\n",
    "\n",
    "        # Oracle speaker num\n",
    "        self._cluster_params.oracle_num_speakers = True\n",
    "\n",
    "        torch.set_default_device(self._cfg.device)\n",
    "\n",
    "    def diarize(self, paths2audio_files: List[str], durations: List, num_speakers: List[int] = None, output_label: bool = True, batch_size: int = 0):\n",
    "        \"\"\"\n",
    "        Diarize list of audio files in paths2audio_files\n",
    "        Arguments:\n",
    "            * paths2audio_files: list of audio files to be diarized\n",
    "            * duration: duration of each audio file\n",
    "            * num_speakers: if not None, then it corresponds to number of speakers in each of the audio input files\n",
    "            * output_label: if True, saves text label file that can be used in Audacity application for visualization\n",
    "            * batch_size: batch_sizeconsidered for extraction of speaker embedding and VAD computation\n",
    "        \"\"\"\n",
    "        # setup manifest file\n",
    "        config_setup(paths2audio_files, self._diarizer_params.manifest_filepath, durations, num_speakers=num_speakers)\n",
    "\n",
    "        self._cluster_params.oracle_num_speakers = num_speakers is not None\n",
    "\n",
    "        self._out_dir = self._diarizer_params.out_dir\n",
    "\n",
    "        self._speaker_dir = os.path.join(self._diarizer_params.out_dir, 'speaker_outputs')\n",
    "\n",
    "        if os.path.exists(self._speaker_dir):\n",
    "            logging.warning(\"Deleting previous clustering diarizer outputs.\")\n",
    "            shutil.rmtree(self._speaker_dir, ignore_errors=True)\n",
    "        os.makedirs(self._speaker_dir)\n",
    "\n",
    "        if not os.path.exists(self._out_dir):\n",
    "            os.mkdir(self._out_dir)\n",
    "\n",
    "        self._vad_dir = os.path.join(self._out_dir, 'vad_outputs')\n",
    "        self._vad_out_file = os.path.join(self._vad_dir, \"vad_out.json\")\n",
    "\n",
    "        if batch_size:\n",
    "            self._cfg.batch_size = batch_size\n",
    "\n",
    "        if paths2audio_files:\n",
    "            if type(paths2audio_files) is list:\n",
    "                self._diarizer_params.manifest_filepath = os.path.join(self._out_dir, 'paths2audio_filepath.json')\n",
    "                config_setup(paths2audio_files, self._diarizer_params.manifest_filepath, durations, num_speakers=num_speakers)\n",
    "                # self.path2audio_files_to_manifest(paths2audio_files, self._diarizer_params.manifest_filepath)\n",
    "            else:\n",
    "                raise ValueError(\"paths2audio_files must be of type list of paths to file containing audio file\")\n",
    "\n",
    "        self.AUDIO_RTTM_MAP = audio_rttm_map(self._diarizer_params.manifest_filepath)\n",
    "\n",
    "        out_rttm_dir = os.path.join(self._out_dir, 'pred_rttms')\n",
    "        os.makedirs(out_rttm_dir, exist_ok=True)\n",
    "\n",
    "        # Speech Activity Detection\n",
    "        self._perform_speech_activity_detection()\n",
    "\n",
    "        # Segmentation\n",
    "        scales = self.multiscale_args_dict['scale_dict'].items()\n",
    "        for scale_idx, (window, shift) in scales:\n",
    "\n",
    "            # Segmentation for the current scale (scale_idx)\n",
    "            self._run_segmentation(window, shift, scale_tag=f'_scale{scale_idx}')\n",
    "\n",
    "            # Embedding Extraction for the current scale (scale_idx)\n",
    "            self._extract_embeddings(self.subsegments_manifest_path, scale_idx, len(scales))\n",
    "\n",
    "            self.multiscale_embeddings_and_timestamps[scale_idx] = [self.embeddings, self.time_stamps]\n",
    "\n",
    "        embs_and_timestamps = get_embs_and_timestamps(\n",
    "            self.multiscale_embeddings_and_timestamps, self.multiscale_args_dict\n",
    "        )\n",
    "\n",
    "        # Clustering\n",
    "        all_reference, all_hypothesis = perform_clustering(\n",
    "            embs_and_timestamps=embs_and_timestamps,\n",
    "            AUDIO_RTTM_MAP=self.AUDIO_RTTM_MAP,\n",
    "            out_rttm_dir=out_rttm_dir,\n",
    "            clustering_params=self._cluster_params,\n",
    "            device=self._speaker_model.device,\n",
    "            verbose=self.verbose,\n",
    "        )\n",
    "        logging.info(\"Outputs are saved in {} directory\".format(os.path.abspath(self._diarizer_params.out_dir)))\n",
    "\n",
    "\n",
    "        # generate label file\n",
    "        if output_label:\n",
    "            for uniq_id, audio_rttm_values in self.AUDIO_RTTM_MAP.items():\n",
    "            \n",
    "                filename = audio_rttm_values.get('audio_filepath').split('/')[-1][:-4]\n",
    "                labels = rttm_to_labels(os.path.join(self.rttm_dir, f'{filename}.rttm'))\n",
    "                hypothesis = labels_to_pyannote_object(labels)\n",
    "\n",
    "                last_label = {\n",
    "                    'start' : None, 'end' : None, 'label' : None\n",
    "                }\n",
    "\n",
    "                with open(os.path.join(self.json_dir, f'{filename}_labels.txt'), 'w') as f:\n",
    "                    for segment, track, label in hypothesis.itertracks(yield_label=True):\n",
    "                        start, end = segment.start, segment.end\n",
    "                        if label == last_label['label']:\n",
    "                            last_label['end'] = end\n",
    "                            continue\n",
    "                        # write previous label\n",
    "                        if last_label['label'] is not None:\n",
    "                            f.write(f\"{last_label['start']}\\t{last_label['end']}\\t{last_label['label']}\\n\")\n",
    "                        last_label = {\n",
    "                            'start' : start, 'end' : end, 'label' : label\n",
    "                        }\n",
    "                    f.write(f\"{last_label['start']}\\t{last_label['end']}\\t{last_label['label']}\\n\")\n",
    "\n",
    "    def _run_vad(self, manifest_file):\n",
    "        \"\"\"\n",
    "        Run voice activity detection. \n",
    "        Get log probability of voice activity detection and smoothes using the post processing parameters. \n",
    "        Using generated frame level predictions generated manifest file for later speaker embedding extraction.\n",
    "        input:\n",
    "        manifest_file (str) : Manifest file containing path to audio file and label as infer\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        shutil.rmtree(self._vad_dir, ignore_errors=True)\n",
    "        os.makedirs(self._vad_dir)\n",
    "\n",
    "        self._vad_model.eval()\n",
    "\n",
    "        time_unit = int(self._vad_window_length_in_sec / self._vad_shift_length_in_sec)\n",
    "        trunc = int(time_unit / 2)\n",
    "        trunc_l = time_unit - trunc\n",
    "        all_len = 0\n",
    "        data = []\n",
    "        for line in open(manifest_file, 'r', encoding='utf-8'):\n",
    "            file = json.loads(line)['audio_filepath']\n",
    "            data.append(get_uniqname_from_filepath(file))\n",
    "\n",
    "        status = get_vad_stream_status(data)\n",
    "        for i, test_batch in enumerate(\n",
    "            tqdm(self._vad_model.test_dataloader(), desc='vad', leave=True, disable=not self.verbose)\n",
    "        ):\n",
    "            test_batch = [x.to(self._vad_model.device) for x in test_batch]\n",
    "            with autocast():\n",
    "                log_probs = self._vad_model(input_signal=test_batch[0], input_signal_length=test_batch[1])\n",
    "                probs = torch.softmax(log_probs, dim=-1)\n",
    "                pred = probs[:, 1]\n",
    "                if status[i] == 'start':\n",
    "                    to_save = pred[:-trunc]\n",
    "                elif status[i] == 'next':\n",
    "                    to_save = pred[trunc:-trunc_l]\n",
    "                elif status[i] == 'end':\n",
    "                    to_save = pred[trunc_l:]\n",
    "                else:\n",
    "                    to_save = pred\n",
    "                all_len += len(to_save)\n",
    "                outpath = os.path.join(self._vad_dir, data[i] + \".frame\")\n",
    "                with open(outpath, \"a\", encoding='utf-8') as fout:\n",
    "                    for f in range(len(to_save)):\n",
    "                        fout.write('{0:0.4f}\\n'.format(to_save[f]))\n",
    "            del test_batch\n",
    "            if status[i] == 'end' or status[i] == 'single':\n",
    "                all_len = 0\n",
    "\n",
    "        if not self._vad_params.smoothing:\n",
    "            # Shift the window by 10ms to generate the frame and use the prediction of the window to represent the label for the frame;\n",
    "            self.vad_pred_dir = self._vad_dir\n",
    "            frame_length_in_sec = self._vad_shift_length_in_sec\n",
    "        else:\n",
    "            # Generate predictions with overlapping input segments. Then a smoothing filter is applied to decide the label for a frame spanned by multiple segments.\n",
    "            # smoothing_method would be either in majority vote (median) or average (mean)\n",
    "            logging.info(\"Generating predictions with overlapping input segments\")\n",
    "            smoothing_pred_dir = generate_overlap_vad_seq(\n",
    "                frame_pred_dir=self._vad_dir,\n",
    "                smoothing_method=self._vad_params.smoothing,\n",
    "                overlap=self._vad_params.overlap,\n",
    "                window_length_in_sec=self._vad_window_length_in_sec,\n",
    "                shift_length_in_sec=self._vad_shift_length_in_sec,\n",
    "                num_workers=self._cfg.num_workers,\n",
    "            )\n",
    "            self.vad_pred_dir = smoothing_pred_dir\n",
    "            frame_length_in_sec = 0.01\n",
    "\n",
    "        logging.info(\"Converting frame level prediction to speech/no-speech segment in start and end times format.\")\n",
    "\n",
    "        vad_params = self._vad_params if isinstance(self._vad_params, (DictConfig, dict)) else self._vad_params.dict()\n",
    "        table_out_dir = generate_vad_segment_table(\n",
    "            vad_pred_dir=self.vad_pred_dir,\n",
    "            postprocessing_params=vad_params,\n",
    "            frame_length_in_sec=frame_length_in_sec,\n",
    "            num_workers=self._cfg.num_workers,\n",
    "            out_dir=self._vad_dir,\n",
    "        )\n",
    "\n",
    "        AUDIO_VAD_RTTM_MAP = {}\n",
    "        for key in self.AUDIO_RTTM_MAP:\n",
    "            if os.path.exists(os.path.join(table_out_dir, key + \".txt\")):\n",
    "                AUDIO_VAD_RTTM_MAP[key] = deepcopy(self.AUDIO_RTTM_MAP[key])\n",
    "                AUDIO_VAD_RTTM_MAP[key]['rttm_filepath'] = os.path.join(table_out_dir, key + \".txt\")\n",
    "            else:\n",
    "                logging.warning(f\"no vad file found for {key} due to zero or negative duration\")\n",
    "\n",
    "        write_rttm2manifest(AUDIO_VAD_RTTM_MAP, self._vad_out_file)\n",
    "        self._speaker_manifest_path = self._vad_out_file\n",
    "\n",
    "    def _extract_embeddings(self, manifest_file: str, scale_idx: int, num_scales: int):\n",
    "        \"\"\"\n",
    "        This method extracts speaker embeddings from segments passed through manifest_file\n",
    "        Optionally you may save the intermediate speaker embeddings for debugging or any use. \n",
    "        \"\"\"\n",
    "        logging.info(\"Extracting embeddings for Diarization\")\n",
    "        self._setup_spkr_test_data(manifest_file)\n",
    "        self.embeddings = {}\n",
    "        self._speaker_model.eval()\n",
    "        self.time_stamps = {}\n",
    "\n",
    "        all_embs = torch.empty([0]).cpu()\n",
    "        for test_batch in tqdm(\n",
    "            self._speaker_model.test_dataloader(),\n",
    "            desc=f'[{scale_idx+1}/{num_scales}] extract embeddings',\n",
    "            leave=True,\n",
    "            disable=not self.verbose,\n",
    "        ):\n",
    "            test_batch = [x.to(self._speaker_model.device) for x in test_batch]\n",
    "            audio_signal, audio_signal_len, labels, slices = test_batch\n",
    "            with autocast():\n",
    "                _, embs = self._speaker_model.forward(input_signal=audio_signal, input_signal_length=audio_signal_len)\n",
    "                emb_shape = embs.shape[-1]\n",
    "                embs = embs.view(-1, emb_shape)\n",
    "                all_embs = torch.cat((all_embs, embs.cpu().detach()), dim=0)\n",
    "            del test_batch\n",
    "\n",
    "        with open(manifest_file, 'r', encoding='utf-8') as manifest:\n",
    "            for i, line in enumerate(manifest.readlines()):\n",
    "                line = line.strip()\n",
    "                dic = json.loads(line)\n",
    "                uniq_name = get_uniqname_from_filepath(dic['audio_filepath'])\n",
    "                if uniq_name in self.embeddings:\n",
    "                    self.embeddings[uniq_name] = torch.cat((self.embeddings[uniq_name], all_embs[i].view(1, -1)))\n",
    "                else:\n",
    "                    self.embeddings[uniq_name] = all_embs[i].view(1, -1)\n",
    "                if uniq_name not in self.time_stamps:\n",
    "                    self.time_stamps[uniq_name] = []\n",
    "                start = dic['offset']\n",
    "                end = start + dic['duration']\n",
    "                self.time_stamps[uniq_name].append([start, end])\n",
    "\n",
    "        if self._speaker_params.save_embeddings:\n",
    "            embedding_dir = os.path.join(self._speaker_dir, 'embeddings')\n",
    "            if not os.path.exists(embedding_dir):\n",
    "                os.makedirs(embedding_dir, exist_ok=True)\n",
    "\n",
    "            prefix = get_uniqname_from_filepath(manifest_file)\n",
    "            name = os.path.join(embedding_dir, prefix)\n",
    "            self._embeddings_file = name + f'_embeddings.pkl'\n",
    "            pkl.dump(self.embeddings, open(self._embeddings_file, 'wb'))\n",
    "            logging.info(\"Saved embedding files to {}\".format(embedding_dir))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Model\n",
    "\n",
    "We first set configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_id = 1\n",
    "MODEL_CONFIG = os.path.join('config','model_config.yaml')\n",
    "config = OmegaConf.load(MODEL_CONFIG)\n",
    "config.device = f'cuda:{device_id}'\n",
    "config.verbose = True\n",
    "model = NeMoDiarizer(cfg=config)\n",
    "torch.set_default_device(config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_dir = 'conversations'\n",
    "filenames = []\n",
    "for file in os.listdir(audio_dir):\n",
    "    if file[-4:] == '.wav':\n",
    "        filenames.append(os.path.join(audio_dir, file))\n",
    "\n",
    "# get duration\n",
    "durations = []\n",
    "\n",
    "# get number of speakers\n",
    "num_speakers = []\n",
    "\n",
    "for filename in filenames:\n",
    "    json_path = f'{filename[:-4]}.json'\n",
    "    raw_json = json.loads(open(json_path, 'r').read())\n",
    "    num_speaker = len(raw_json['participants'])\n",
    "    # for p in raw_json['participants']:\n",
    "    #     if p['name'] in ('Hearth', 'participant', 'Participant'):\n",
    "    #         continue\n",
    "    num_speakers.append(num_speaker)\n",
    "    durations.append(raw_json['duration'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "splitting manifest: 100%|██████████| 21/21 [00:10<00:00,  1.97it/s]\n",
      "vad: 100%|██████████| 1760/1760 [06:12<00:00,  4.73it/s]\n",
      "                                                                 \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiarize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdurations\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/diarization/model/NeMo_diarizer.py:122\u001b[0m, in \u001b[0;36mNeMoDiarizer.diarize\u001b[0;34m(self, paths2audio_files, durations, num_speakers, output_label, batch_size)\u001b[0m\n\u001b[1;32m    119\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(out_rttm_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# Speech Activity Detection\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_perform_speech_activity_detection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# Segmentation\u001b[39;00m\n\u001b[1;32m    125\u001b[0m scales \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultiscale_args_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscale_dict\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mitems()\n",
      "File \u001b[0;32m~/miniconda3/envs/nemo/lib/python3.10/site-packages/nemo/collections/asr/models/clustering_diarizer.py:325\u001b[0m, in \u001b[0;36mClusteringDiarizer._perform_speech_activity_detection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    320\u001b[0m         logging\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    321\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you encounter CUDA memory issue, try splitting manifest entry by split_duration to avoid it.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m         )\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setup_vad_test_data(manifest_vad_input)\n\u001b[0;32m--> 325\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_vad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmanifest_vad_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_diarizer_params\u001b[38;5;241m.\u001b[39mvad\u001b[38;5;241m.\u001b[39mexternal_vad_manifest \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_speaker_manifest_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_diarizer_params\u001b[38;5;241m.\u001b[39mvad\u001b[38;5;241m.\u001b[39mexternal_vad_manifest\n",
      "File \u001b[0;32m~/diarization/model/NeMo_diarizer.py:236\u001b[0m, in \u001b[0;36mNeMoDiarizer._run_vad\u001b[0;34m(self, manifest_file)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;66;03m# Generate predictions with overlapping input segments. Then a smoothing filter is applied to decide the label for a frame spanned by multiple segments.\u001b[39;00m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# smoothing_method would be either in majority vote (median) or average (mean)\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating predictions with overlapping input segments\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 236\u001b[0m     smoothing_pred_dir \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_overlap_vad_seq\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe_pred_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_vad_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43msmoothing_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_vad_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msmoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverlap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_vad_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moverlap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwindow_length_in_sec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_vad_window_length_in_sec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshift_length_in_sec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_vad_shift_length_in_sec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvad_pred_dir \u001b[38;5;241m=\u001b[39m smoothing_pred_dir\n\u001b[1;32m    245\u001b[0m     frame_length_in_sec \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m\n",
      "File \u001b[0;32m~/diarization/util/nemo_util.py:127\u001b[0m, in \u001b[0;36mgenerate_overlap_vad_seq\u001b[0;34m(frame_pred_dir, smoothing_method, overlap, window_length_in_sec, shift_length_in_sec, num_workers, out_dir)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m frame_filepath \u001b[38;5;129;01min\u001b[39;00m tqdm(frame_filepathlist, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerating preds\u001b[39m\u001b[38;5;124m'\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 127\u001b[0m         \u001b[43mgenerate_overlap_vad_seq_per_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_filepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mper_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m overlap_out_dir\n",
      "File \u001b[0;32m~/diarization/util/nemo_util.py:146\u001b[0m, in \u001b[0;36mgenerate_overlap_vad_seq_per_file\u001b[0;34m(frame_filepath, per_args)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(per_args[i]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(per_args[i]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m    144\u001b[0m         per_args_float[i] \u001b[38;5;241m=\u001b[39m per_args[i]\n\u001b[0;32m--> 146\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_overlap_vad_seq_per_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mper_args_float\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msmoothing_method\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m overlap_filepath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(out_dir, name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m smoothing_method)\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(overlap_filepath, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/diarization/util/nemo_util.py:227\u001b[0m, in \u001b[0;36mgenerate_overlap_vad_seq_per_tensor\u001b[0;34m(frame, per_args, smoothing_method)\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m j \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m target_len \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    225\u001b[0m             preds[j] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((preds[j], og_pred\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)), \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 227\u001b[0m preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([torch\u001b[38;5;241m.\u001b[39mnanquantile(l, q\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m preds])\n\u001b[1;32m    228\u001b[0m nan_idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39misnan(preds)\n\u001b[1;32m    229\u001b[0m last_non_nan_pred \u001b[38;5;241m=\u001b[39m preds[\u001b[38;5;241m~\u001b[39mnan_idx][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/diarization/util/nemo_util.py:227\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m j \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m target_len \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    225\u001b[0m             preds[j] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((preds[j], og_pred\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)), \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 227\u001b[0m preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnanquantile\u001b[49m\u001b[43m(\u001b[49m\u001b[43ml\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m preds])\n\u001b[1;32m    228\u001b[0m nan_idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39misnan(preds)\n\u001b[1;32m    229\u001b[0m last_non_nan_pred \u001b[38;5;241m=\u001b[39m preds[\u001b[38;5;241m~\u001b[39mnan_idx][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/nemo/lib/python3.10/site-packages/torch/utils/_device.py:77\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.diarize(filenames, durations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/shiminki/.local/lib/python3.10/site-packages/pyannote/metrics/utils.py:200: UserWarning: 'uem' was approximated by the union of 'reference' and 'hypothesis' extents.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "diarize_performance = {}\n",
    "for file in filenames:\n",
    "    # Merge discontinued labels\n",
    "    merged_label = Annotation()\n",
    "\n",
    "    filename = file.split('/')[-1]\n",
    "\n",
    "    with open(os.path.join('outputs', 'pred_json', f'{filename[:-4]}_labels.txt')) as f:\n",
    "        for line in f.readlines():\n",
    "            start, end, speaker = line.split()\n",
    "            start, end = float(start), float(end)\n",
    "            merged_label[Segment(start, end)] = speaker\n",
    "\n",
    "    # Evaluate metrics using merged label\n",
    "    true_labels = rttm_to_labels(os.path.join(audio_dir, f'{filename[:-4]}.rttm'))\n",
    "    reference = labels_to_pyannote_object(true_labels)\n",
    "\n",
    "    performance = DiarizationErrorRate().compute_components(reference, merged_label)\n",
    "    metrics = ['confusion', 'missed detection', 'false alarm']\n",
    "    for metric in metrics:\n",
    "        performance[metric] /= performance['total']\n",
    "    performance['DER'] = sum(performance[metric] for metric in metrics)\n",
    "    diarize_performance[filename] = performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = {x : [] for x in performance}\n",
    "index = []\n",
    "\n",
    "for filename in diarize_performance:\n",
    "    for x in diarize_performance[filename]:\n",
    "        df[x].append(diarize_performance[filename][x])\n",
    "    index.append(filename)\n",
    "\n",
    "df = pd.DataFrame(df, index=index)\n",
    "\n",
    "df.to_csv('diarize_performance_no_oracle.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>confusion</th>\n",
       "      <th>total</th>\n",
       "      <th>correct</th>\n",
       "      <th>false alarm</th>\n",
       "      <th>missed detection</th>\n",
       "      <th>DER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>conversation-81.wav</th>\n",
       "      <td>0.020155</td>\n",
       "      <td>4999.4</td>\n",
       "      <td>4681.312</td>\n",
       "      <td>0.023827</td>\n",
       "      <td>0.043470</td>\n",
       "      <td>0.087452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conversation-70.wav</th>\n",
       "      <td>0.050032</td>\n",
       "      <td>2015.3</td>\n",
       "      <td>1825.400</td>\n",
       "      <td>0.021431</td>\n",
       "      <td>0.044197</td>\n",
       "      <td>0.115660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conversation-40.wav</th>\n",
       "      <td>0.045632</td>\n",
       "      <td>4483.7</td>\n",
       "      <td>4181.480</td>\n",
       "      <td>0.017209</td>\n",
       "      <td>0.021772</td>\n",
       "      <td>0.084613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conversation-67.wav</th>\n",
       "      <td>0.054663</td>\n",
       "      <td>5250.7</td>\n",
       "      <td>4700.945</td>\n",
       "      <td>0.027641</td>\n",
       "      <td>0.050038</td>\n",
       "      <td>0.132342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conversation-42.wav</th>\n",
       "      <td>0.023578</td>\n",
       "      <td>5551.2</td>\n",
       "      <td>5240.178</td>\n",
       "      <td>0.025278</td>\n",
       "      <td>0.032450</td>\n",
       "      <td>0.081306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conversation-80.wav</th>\n",
       "      <td>0.019234</td>\n",
       "      <td>3192.0</td>\n",
       "      <td>3000.495</td>\n",
       "      <td>0.013515</td>\n",
       "      <td>0.040761</td>\n",
       "      <td>0.073510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conversation-41.wav</th>\n",
       "      <td>0.042411</td>\n",
       "      <td>3882.2</td>\n",
       "      <td>3586.005</td>\n",
       "      <td>0.030042</td>\n",
       "      <td>0.033885</td>\n",
       "      <td>0.106338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conversation-7.wav</th>\n",
       "      <td>0.051628</td>\n",
       "      <td>2585.4</td>\n",
       "      <td>2321.580</td>\n",
       "      <td>0.011832</td>\n",
       "      <td>0.050414</td>\n",
       "      <td>0.113874</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     confusion   total   correct  false alarm  \\\n",
       "conversation-81.wav   0.020155  4999.4  4681.312     0.023827   \n",
       "conversation-70.wav   0.050032  2015.3  1825.400     0.021431   \n",
       "conversation-40.wav   0.045632  4483.7  4181.480     0.017209   \n",
       "conversation-67.wav   0.054663  5250.7  4700.945     0.027641   \n",
       "conversation-42.wav   0.023578  5551.2  5240.178     0.025278   \n",
       "conversation-80.wav   0.019234  3192.0  3000.495     0.013515   \n",
       "conversation-41.wav   0.042411  3882.2  3586.005     0.030042   \n",
       "conversation-7.wav    0.051628  2585.4  2321.580     0.011832   \n",
       "\n",
       "                     missed detection       DER  \n",
       "conversation-81.wav          0.043470  0.087452  \n",
       "conversation-70.wav          0.044197  0.115660  \n",
       "conversation-40.wav          0.021772  0.084613  \n",
       "conversation-67.wav          0.050038  0.132342  \n",
       "conversation-42.wav          0.032450  0.081306  \n",
       "conversation-80.wav          0.040761  0.073510  \n",
       "conversation-41.wav          0.033885  0.106338  \n",
       "conversation-7.wav           0.050414  0.113874  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"DER\"] <= 0.15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"diarize_performance.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>total</th>\n",
       "      <th>missed detection</th>\n",
       "      <th>false alarm</th>\n",
       "      <th>correct</th>\n",
       "      <th>confusion</th>\n",
       "      <th>DER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>conversation-55.wav</td>\n",
       "      <td>5908.20</td>\n",
       "      <td>0.104163</td>\n",
       "      <td>0.016196</td>\n",
       "      <td>4691.321</td>\n",
       "      <td>0.101801</td>\n",
       "      <td>0.222160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>conversation-65.wav</td>\n",
       "      <td>5459.80</td>\n",
       "      <td>0.030440</td>\n",
       "      <td>0.029127</td>\n",
       "      <td>4586.261</td>\n",
       "      <td>0.129554</td>\n",
       "      <td>0.189121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>conversation-79.wav</td>\n",
       "      <td>2686.90</td>\n",
       "      <td>0.088853</td>\n",
       "      <td>0.021623</td>\n",
       "      <td>2164.915</td>\n",
       "      <td>0.105417</td>\n",
       "      <td>0.215894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>conversation-66.wav</td>\n",
       "      <td>5044.90</td>\n",
       "      <td>0.031653</td>\n",
       "      <td>0.027466</td>\n",
       "      <td>4351.357</td>\n",
       "      <td>0.105821</td>\n",
       "      <td>0.164940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>conversation-73.wav</td>\n",
       "      <td>2866.30</td>\n",
       "      <td>0.138907</td>\n",
       "      <td>0.062837</td>\n",
       "      <td>1595.220</td>\n",
       "      <td>0.304549</td>\n",
       "      <td>0.506294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>conversation-46.wav</td>\n",
       "      <td>4386.70</td>\n",
       "      <td>0.044642</td>\n",
       "      <td>0.030160</td>\n",
       "      <td>2704.770</td>\n",
       "      <td>0.338773</td>\n",
       "      <td>0.413576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>conversation-39.wav</td>\n",
       "      <td>3350.16</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.102231</td>\n",
       "      <td>1828.230</td>\n",
       "      <td>0.454283</td>\n",
       "      <td>0.556517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>conversation-82.wav</td>\n",
       "      <td>4582.10</td>\n",
       "      <td>0.023462</td>\n",
       "      <td>0.015482</td>\n",
       "      <td>3869.370</td>\n",
       "      <td>0.132084</td>\n",
       "      <td>0.171029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>conversation-57.wav</td>\n",
       "      <td>5895.60</td>\n",
       "      <td>0.025938</td>\n",
       "      <td>0.014189</td>\n",
       "      <td>4341.423</td>\n",
       "      <td>0.237679</td>\n",
       "      <td>0.277805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Unnamed: 0    total  missed detection  false alarm   correct  \\\n",
       "3   conversation-55.wav  5908.20          0.104163     0.016196  4691.321   \n",
       "5   conversation-65.wav  5459.80          0.030440     0.029127  4586.261   \n",
       "8   conversation-79.wav  2686.90          0.088853     0.021623  2164.915   \n",
       "9   conversation-66.wav  5044.90          0.031653     0.027466  4351.357   \n",
       "10  conversation-73.wav  2866.30          0.138907     0.062837  1595.220   \n",
       "12  conversation-46.wav  4386.70          0.044642     0.030160  2704.770   \n",
       "15  conversation-39.wav  3350.16          0.000003     0.102231  1828.230   \n",
       "16  conversation-82.wav  4582.10          0.023462     0.015482  3869.370   \n",
       "18  conversation-57.wav  5895.60          0.025938     0.014189  4341.423   \n",
       "\n",
       "    confusion       DER  \n",
       "3    0.101801  0.222160  \n",
       "5    0.129554  0.189121  \n",
       "8    0.105417  0.215894  \n",
       "9    0.105821  0.164940  \n",
       "10   0.304549  0.506294  \n",
       "12   0.338773  0.413576  \n",
       "15   0.454283  0.556517  \n",
       "16   0.132084  0.171029  \n",
       "18   0.237679  0.277805  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"confusion\"] > 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>confusion</th>\n",
       "      <th>total</th>\n",
       "      <th>correct</th>\n",
       "      <th>false alarm</th>\n",
       "      <th>missed detection</th>\n",
       "      <th>DER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>21.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>21.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.128494</td>\n",
       "      <td>3955.793333</td>\n",
       "      <td>3257.507619</td>\n",
       "      <td>0.027606</td>\n",
       "      <td>0.065733</td>\n",
       "      <td>0.221833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.121550</td>\n",
       "      <td>1443.440785</td>\n",
       "      <td>1331.835534</td>\n",
       "      <td>0.016045</td>\n",
       "      <td>0.041605</td>\n",
       "      <td>0.153253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.019234</td>\n",
       "      <td>244.800000</td>\n",
       "      <td>113.915000</td>\n",
       "      <td>0.009092</td>\n",
       "      <td>0.021772</td>\n",
       "      <td>0.073510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.045632</td>\n",
       "      <td>2866.300000</td>\n",
       "      <td>2321.580000</td>\n",
       "      <td>0.015528</td>\n",
       "      <td>0.033885</td>\n",
       "      <td>0.113874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.085201</td>\n",
       "      <td>4036.500000</td>\n",
       "      <td>3105.945000</td>\n",
       "      <td>0.023458</td>\n",
       "      <td>0.050038</td>\n",
       "      <td>0.174643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.130677</td>\n",
       "      <td>5044.900000</td>\n",
       "      <td>4593.031000</td>\n",
       "      <td>0.030042</td>\n",
       "      <td>0.088008</td>\n",
       "      <td>0.218585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.439641</td>\n",
       "      <td>5908.200000</td>\n",
       "      <td>5240.178000</td>\n",
       "      <td>0.066113</td>\n",
       "      <td>0.169649</td>\n",
       "      <td>0.546385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       confusion        total      correct  false alarm  missed detection  \\\n",
       "count  21.000000    21.000000    21.000000    21.000000         21.000000   \n",
       "mean    0.128494  3955.793333  3257.507619     0.027606          0.065733   \n",
       "std     0.121550  1443.440785  1331.835534     0.016045          0.041605   \n",
       "min     0.019234   244.800000   113.915000     0.009092          0.021772   \n",
       "25%     0.045632  2866.300000  2321.580000     0.015528          0.033885   \n",
       "50%     0.085201  4036.500000  3105.945000     0.023458          0.050038   \n",
       "75%     0.130677  5044.900000  4593.031000     0.030042          0.088008   \n",
       "max     0.439641  5908.200000  5240.178000     0.066113          0.169649   \n",
       "\n",
       "             DER  \n",
       "count  21.000000  \n",
       "mean    0.221833  \n",
       "std     0.153253  \n",
       "min     0.073510  \n",
       "25%     0.113874  \n",
       "50%     0.174643  \n",
       "75%     0.218585  \n",
       "max     0.546385  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nemo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
